{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyM71UF+wMxSkLVejbt47aJ3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Предыдущий файл - 3_data_from_SMILES_to_text.ipynb"],"metadata":{"id":"O8HF6fTVzHu2"}},{"cell_type":"markdown","source":["Теперь необходимо создать тренировочный и тестовый наборы данных в виде чата.\n","Каждый из блоков кода я отдельно запускал на своем компьютере и кластерном сервере в формате .py для ускорения создания наборов данных."],"metadata":{"id":"aiyf4tcOwI2U"}},{"cell_type":"markdown","source":["#Тестовый набор данных"],"metadata":{"id":"T2pzZ_pTww4p"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from datasets import Dataset\n","from tqdm import tqdm\n","\n","ds = pd.read_csv(\"text_chem_data.csv\", sep=\"\\t\", index_col=0) #загрузка данных\n","\n","train_df, test_df = train_test_split(ds, test_size=0.3, random_state=42) #разделение данных\n","\n","model_name = \"KingNish/Reasoning-Llama-1b-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\")\n","\n","special_tokens_dict = {'additional_special_tokens': ['<|result|>']} #добавляем специальный токен, чтобы модель понимала, где должен находиться ответ\n","tokenizer.add_special_tokens(special_tokens_dict)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","def generate_shot_sequence(train_df, test_example, num_shot: int): #функция для генерации шотов. Примеры берутся из тренировочного набора данных, реакции для задачи - из тестового. При этом примеры берутся разнообразные, если есть реакция с 'low' выходом, то следующая будет с 'high'\n","    high_samples = train_df[train_df[\"Yield\"] == \"high\"].sample(n=num_shot // 2, replace=True)\n","    low_samples = train_df[train_df[\"Yield\"] == \"low\"].sample(n=num_shot // 2, replace=True)\n","    shots = pd.concat([high_samples, low_samples]).sample(frac=1, random_state=42)\n","\n","    messages = [\n","        {'role': 'system', 'content': 'Here are some examples of chemical reactions and their yield rates. \"high\" means the yield is >= 70%. \"low\" means the yield is <70%.'}\n","    ]\n","\n","    for i, shot in enumerate(shots.to_dict(orient=\"records\"), 1):\n","        messages.append({\"role\": \"user\", \"content\": f'Example {i}: Reaction: {shot[\"text\"]}'})\n","        messages.append({\"role\": \"assistant\", \"content\": shot[\"Yield\"]})\n","\n","    messages.append({\"role\": \"user\", \"content\": f'Task: Based on these examples, predict the yield of the following reaction. Print \"high\" if yield of this reaction >=70%, or \"low\" if yield is <70%. Reaction: {test_example} Answer: <|result|>'})\n","\n","    return messages\n","\n","def prepare_dataset(train_df, test_df, num_shot=2): #генерация шотов и их токенизация\n","    dataset = []\n","    for test_example in tqdm(test_df[\"text\"]):\n","        shots = generate_shot_sequence(train_df, test_example, num_shot)\n","        text_prompt = tokenizer.apply_chat_template(shots, tokenize=False)\n","        answer = test_df[test_df[\"text\"] == test_example][\"Yield\"].values[0]\n","        dataset.append({\"text\": text_prompt, \"answer\": answer})\n","\n","    return pd.DataFrame(dataset)\n","\n","test_dataset = prepare_dataset(train_df, test_df, num_shot=2)\n","\n","test_dataset.to_csv('test_dataset.csv', sep='\\t')"],"metadata":{"id":"fV0EVgBxwuzU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Первая половина тренировочного набора данных"],"metadata":{"id":"uazMv2BfyFQ6"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from datasets import Dataset\n","from tqdm import tqdm\n","\n","ds = pd.read_csv(\"text_chem_data.csv\", sep=\"\\t\", index_col=0)\n","\n","train_df, test_df = train_test_split(ds, test_size=0.3, random_state=42)\n","\n","half = len(train_df)//2\n","half_df_1 = train_df.iloc[:half]\n","half_df_1 = half_df_1[['text']]\n","half_df_2 = train_df.iloc[half:]\n","half_df_2 = half_df_2[['text']]\n","\n","model_name = \"KingNish/Reasoning-Llama-1b-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\")\n","\n","special_tokens_dict = {'additional_special_tokens': ['<|result|>']}\n","tokenizer.add_special_tokens(special_tokens_dict)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","def generate_shot_sequence(train_df, test_example, num_shot: int):\n","    high_samples = train_df[train_df[\"Yield\"] == \"high\"].sample(n=num_shot // 2, replace=True)\n","    low_samples = train_df[train_df[\"Yield\"] == \"low\"].sample(n=num_shot // 2, replace=True)\n","    shots = pd.concat([high_samples, low_samples]).sample(frac=1, random_state=42)\n","\n","    messages = [\n","        {'role': 'system', 'content': 'Here are some examples of chemical reactions and their yield rates. \"high\" means the yield is >= 70%. \"low\" means the yield is <70%.'}\n","    ]\n","\n","    for i, shot in enumerate(shots.to_dict(orient=\"records\"), 1):\n","        messages.append({\"role\": \"user\", \"content\": f'Example {i}: Reaction: {shot[\"text\"]}'})\n","        messages.append({\"role\": \"assistant\", \"content\": shot[\"Yield\"]})\n","\n","    messages.append({\"role\": \"user\", \"content\": f'Task: Based on these examples, predict the yield of the following reaction. Print \"high\" if yield of this reaction >=70%, or \"low\" if yield is <70%. Reaction: {test_example} Answer: <|result|>'})\n","\n","    return messages\n","\n","def prepare_dataset(train_df, test_df, num_shot=2):\n","    dataset = []\n","    for test_example in tqdm(test_df[\"text\"]):\n","        shots = generate_shot_sequence(train_df, test_example, num_shot)\n","        text_prompt = tokenizer.apply_chat_template(shots, tokenize=False)\n","        answer = test_df[test_df[\"text\"] == test_example][\"Yield\"].values[0]\n","        dataset.append({\"text\": text_prompt, \"answer\": answer})\n","\n","    return pd.DataFrame(dataset)\n","\n","train_dataset = prepare_dataset(train_df, half_df_1, num_shot=2)\n","\n","train_dataset.to_csv('train_dataset_1.csv', sep='\\t')"],"metadata":{"id":"HHga5RLtyJgL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Вторая половина тренировочного набора данных"],"metadata":{"id":"EvnVy17vygtK"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from datasets import Dataset\n","from tqdm import tqdm\n","\n","ds = pd.read_csv(\"text_chem_data.csv\", sep=\"\\t\", index_col=0)\n","\n","train_df, test_df = train_test_split(ds, test_size=0.3, random_state=42)\n","\n","half = len(train_df)//2\n","half_df_1 = train_df.iloc[:half]\n","half_df_1 = half_df_1[['text']]\n","half_df_2 = train_df.iloc[half:]\n","half_df_2 = half_df_2[['text']]\n","\n","model_name = \"KingNish/Reasoning-Llama-1b-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\")\n","\n","special_tokens_dict = {'additional_special_tokens': ['<|result|>']}\n","tokenizer.add_special_tokens(special_tokens_dict)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","def generate_shot_sequence(train_df, test_example, num_shot: int):\n","    high_samples = train_df[train_df[\"Yield\"] == \"high\"].sample(n=num_shot // 2, replace=True)\n","    low_samples = train_df[train_df[\"Yield\"] == \"low\"].sample(n=num_shot // 2, replace=True)\n","    shots = pd.concat([high_samples, low_samples]).sample(frac=1, random_state=42)\n","\n","    messages = [\n","        {'role': 'system', 'content': 'Here are some examples of chemical reactions and their yield rates. \"high\" means the yield is >= 70%. \"low\" means the yield is <70%.'}\n","    ]\n","\n","    for i, shot in enumerate(shots.to_dict(orient=\"records\"), 1):\n","        messages.append({\"role\": \"user\", \"content\": f'Example {i}: Reaction: {shot[\"text\"]}'})\n","        messages.append({\"role\": \"assistant\", \"content\": shot[\"Yield\"]})\n","\n","    messages.append({\"role\": \"user\", \"content\": f'Task: Based on these examples, predict the yield of the following reaction. Print \"high\" if yield of this reaction >=70%, or \"low\" if yield is <70%. Reaction: {test_example} Answer: <|result|>'})\n","\n","    return messages\n","\n","def prepare_dataset(train_df, test_df, num_shot=2):\n","    dataset = []\n","    for test_example in tqdm(test_df[\"text\"]):\n","        shots = generate_shot_sequence(train_df, test_example, num_shot)\n","        text_prompt = tokenizer.apply_chat_template(shots, tokenize=False)\n","        answer = test_df[test_df[\"text\"] == test_example][\"Yield\"].values[0]\n","        dataset.append({\"text\": text_prompt, \"answer\": answer})\n","\n","    return pd.DataFrame(dataset)\n","\n","train_dataset = prepare_dataset(train_df, half_df_2, num_shot=2)\n","\n","train_dataset.to_csv('train_dataset_2.csv', sep='\\t')"],"metadata":{"id":"22ndJLPrys_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Валидационный набор данных"],"metadata":{"id":"YXe2HxWCXj02"}},{"cell_type":"markdown","source":["Данный набор сделан на подобие тестового набора, однако реакции в примерах будут другими"],"metadata":{"id":"1Fx-tMBKXpdW"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","import torch\n","from datasets import Dataset\n","from tqdm import tqdm\n","\n","ds = pd.read_csv(\"text_chem_data.csv\", sep=\"\\t\", index_col=0)\n","\n","train_df, test_df = train_test_split(ds, test_size=0.3, random_state=42)\n","\n","model_name = \"KingNish/Reasoning-Llama-1b-v0.1\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n","tokenizer.padding_side = 'right'\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n","\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    trust_remote_code=True,\n","    quantization_config=bnb_config,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\")\n","\n","special_tokens_dict = {'additional_special_tokens': ['<|result|>']}\n","tokenizer.add_special_tokens(special_tokens_dict)\n","model.resize_token_embeddings(len(tokenizer))\n","\n","def generate_shot_sequence(train_df, test_example, num_shot: int):\n","    high_samples = train_df[train_df[\"Yield\"] == \"high\"].sample(n=num_shot // 2, replace=True)\n","    low_samples = train_df[train_df[\"Yield\"] == \"low\"].sample(n=num_shot // 2, replace=True)\n","    shots = pd.concat([high_samples, low_samples]).sample(frac=1, random_state=42)\n","\n","    messages = [\n","        {'role': 'system', 'content': 'Here are some examples of chemical reactions and their yield rates. \"high\" means the yield is >= 70%. \"low\" means the yield is <70%.'}\n","    ]\n","\n","    for i, shot in enumerate(shots.to_dict(orient=\"records\"), 1):\n","        messages.append({\"role\": \"user\", \"content\": f'Example {i}: Reaction: {shot[\"text\"]}'})\n","        messages.append({\"role\": \"assistant\", \"content\": shot[\"Yield\"]})\n","\n","    messages.append({\"role\": \"user\", \"content\": f'Task: Based on these examples, predict the yield of the following reaction. Print \"high\" if yield of this reaction >=70%, or \"low\" if yield is <70%. Reaction: {test_example} Answer: <|result|>'})\n","\n","    return messages\n","\n","def prepare_dataset(train_df, test_df, num_shot=2):\n","    dataset = []\n","    for test_example in tqdm(test_df[\"text\"]):\n","        shots = generate_shot_sequence(train_df, test_example, num_shot)\n","        text_prompt = tokenizer.apply_chat_template(shots, tokenize=False)\n","        answer = test_df[test_df[\"text\"] == test_example][\"Yield\"].values[0]\n","        dataset.append({\"text\": text_prompt, \"answer\": answer})\n","\n","    return pd.DataFrame(dataset)\n","\n","test_dataset = prepare_dataset(train_df, test_df, num_shot=2)\n","\n","test_dataset.to_csv('valid_dataset.csv', sep='\\t')"],"metadata":{"id":"MQ6T2kqUXm8x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Таким образом, пример одного таска для модели без специальных токенов выглядел так:"],"metadata":{"id":"HRXJRsPXzVKa"}},{"cell_type":"markdown","source":["system:\n","\n","Here are some examples of chemical reactions and their yield rates. \"high\" means the yield is >= 70%. \"low\" means the yield is <70%.\n","\n","user:\n","\n","Example 1: Reaction: 2-(carbamoylamino)-5-(4-ethenylphenyl)-1h-pyrrole-3-carboxamide reacts in the presence of palladium, methanol to produce 2-(carbamoylamino)-5-(4-ethylphenyl)-1h-pyrrole-3-carboxamide.\n","\n","assistant: low\n","\n","user:\n","\n","Example 2: Reaction:\n","Isocyanatomethylbenzene, 5-[5-(azepan-1-ylmethyl)thiophen-2-yl]-1,3,4-oxadiazol-2-amine react together in the presence of pyridine to produce 1-[5-[5-(azepan-1-ylmethyl)thiophen-2-yl]-1,3,4-oxadiazol-2-yl]-3-benzylurea.\n","\n","assistant: high\n","\n","user:\n","\n","Task: Based on these examples, predict the yield of the following reaction. Print \"high\" if yield of this reaction >=70%, or \"low\" if yield is <70%.  Reaction:\n","N-(5-acetamido-2-aminophenyl)-4-tert-butylbenzamide, 1h-indole-6-carboxylic acid, hexafluorophosphate, bromo(tripyrrolidin-1-yl)phosphanium, n-ethyl-n-propan-2-ylpropan-2-amine react together in the presence of dichloromethane, n,n-dimethylformamide to produce n-[4-acetamido-2-[(4-tert-butylbenzoyl)amino]phenyl]-1h-indole-6-carboxamide.\n","Answer:\n"],"metadata":{"id":"ZibOQWlozeHv"}},{"cell_type":"markdown","source":["Продолжение в 5_llama_training.py"],"metadata":{"id":"dxXJupYw0HuK"}}]}